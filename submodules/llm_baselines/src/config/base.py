import argparse
import ast
import torch

import distributed


def parse_args(base_parser, args, namespace):
    parser = base_parser
    # General training params
    parser.add_argument('--batch_size', default=50, type=int)
    parser.add_argument('--acc_steps', default=4, type=int)
    parser.add_argument('--seed', default=0, type=int)
    parser.add_argument('--device', default='cuda:0', type=str)
    parser.add_argument('--iterations', default=15000, type=int)
    parser.add_argument('--warmup_percent', default=0.02, type=float)

    parser.add_argument('--opt', default='adamw', type=str)
    parser.add_argument('--lr', required=True, type=float)
    parser.add_argument('--opt-kwargs', default=dict(), nargs='*', action=ParseKwargs)
    parser.add_argument('--scheduler', default='cos', choices=['linear', 'cos', 'none'])
    parser.add_argument('--scheduler-kwargs', default=dict(), nargs='*', action=ParseKwargs)

    parser.add_argument('--eval_freq', default=200, type=int) # in iterations
    parser.add_argument('--print_freq', default=10, type=int) # in iterations
    parser.add_argument('--results_base_folder', default="./exps", type=str)
    parser.add_argument('--allow_resume', default=True)
    parser.add_argument('--grad_clip', default=0.0, type=float) # default value is 1.0 in NanoGPT
    # Dataset params
    parser.add_argument('--dataset', default='wikitext', choices=['wikitext', "shakespeare-char", 'arxiv', "arxiv2000", "arxiv+wiki", 'openwebtext2'])
    parser.add_argument('--vocab_size', default=50304, type=int)
    parser.add_argument('--data_in_ram', action='store_true') # force the data to RAM, mostly useless except for openwebtext2 
    # Model params
    parser.add_argument('--model', default='base', choices=['base', 'sparse-heads-q'])
    parser.add_argument('--use_pretrained', default="none", type=str) # 'none', 'gpt-2' or a path to the pretraind model
    parser.add_argument('--dropout', default=0.2, type=float)
    parser.add_argument('--n_head', default=12, type=int)
    parser.add_argument('--n_layer', default=24, type=int) # depths in att + ff blocks
    parser.add_argument('--n_embd', default=768, type=int) # embedding size / hidden size ... 
    parser.add_argument('--sequence_length', default=512, type=int)
    parser.add_argument('--dtype', default='bfloat16',
                        type=lambda t: {
                            'float32': torch.float32,
                            'bfloat16': torch.bfloat16,
                            'float16':torch.float16
                        }[t])
    parser.add_argument('--bias', default=False, type=bool)
    parser.add_argument('--no_compile', action='store_true') # if true then model is not compiled
    parser.add_argument('--linear_cfg', default='standard', type=str)
    parser.add_argument('--final_linear_cfg', default='standard', type=str)
    # logging params (WandB)
    parser.add_argument('--wandb', action='store_true') # whether to use wandb or not
    parser.add_argument('--wandb_project', default="my-project", type=str)
    parser.add_argument('--wandb_run_prefix', default="none", type=str) # is added before the autogenerated experiment name
    parser.add_argument('--eval_seq_prefix', default="The history of Switzerland ", type=str) # prefix used to generate sequences

    # Distributed args
    parser.add_argument('--distributed_backend', default=None, type=str, required=False,
                        choices=distributed.registered_backends())  # distributed backend type
    
    # Dataset split
    parser.add_argument('--is-test-split', action=argparse.BooleanOptionalAction)

    # Dynamic Logging
    parser.add_argument('--dynamics-logger-cfg', type=str, default=None, #default='../../shared/utils/base_logger_cfg.yaml', 
                   help='YAML file path with a DynamicsLogger cfg (disabled if falsy)')
    parser.add_argument('--logger_output_dir', default="output dir for logger", type=str)

    return parser.parse_args(args, namespace)


class ParseKwargs(argparse.Action):
    def __call__(self, parser, namespace, values, option_string=None):
        kw = {} # Maybe OrderedDict?
        for value in values:
            try:
                key, value = value.split('=')
            except ValueError as error:
                print(f'{value=}')
                raise error
            try:
                kw[key] = ast.literal_eval(value)
            except ValueError:
                kw[key] = str(value)  # fallback to string (avoid need to escape on command line)
        setattr(namespace, self.dest, kw)